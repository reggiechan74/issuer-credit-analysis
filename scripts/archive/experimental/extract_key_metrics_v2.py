#!/usr/bin/env python3
"""
Phase 2 (ENHANCED v2.0): Targeted Section Extraction

Implements all four optimizations:
1. ‚úÖ Grep-based section indexing (0 LLM tokens)
2. ‚úÖ Section-by-section extraction with validation
3. ‚úÖ Progressive enhancement (expand reads if data missing)
4. ‚úÖ Checkpointing for resumable extraction

Token reduction: ~140K ‚Üí ~15K (89% savings)
No context window issues: Never reads entire files
Guaranteed accuracy: Reads complete sections, validates each
"""

import json
import sys
import argparse
from pathlib import Path
from typing import List, Optional

# Import the new modules
try:
    from extraction_indexer import ExtractionIndexer
    from section_extractor import SectionExtractor
except ImportError:
    # If running from different directory
    sys.path.insert(0, str(Path(__file__).parent))
    from extraction_indexer import ExtractionIndexer
    from section_extractor import SectionExtractor


def create_enhanced_extraction_prompt(
    indexer: ExtractionIndexer,
    issuer_name: str,
    output_path: Path
) -> str:
    """
    Create enhanced extraction prompt that references indexed sections

    Instead of "read the entire file", this tells Claude Code:
    "Read lines 45-124 of file X for balance sheet"

    Args:
        indexer: ExtractionIndexer with section locations
        issuer_name: Name of issuer
        output_path: Where to save JSON

    Returns:
        Compact, targeted extraction prompt
    """

    # Build section-by-section instructions
    section_instructions = []

    for section_name, location in indexer.index.items():
        section_instructions.append(f"""
### Extract {section_name}

**File:** `{location.file}`
**Lines:** {location.start_line} to {location.end_line} (read exactly {location.length} lines)
**Estimated tokens:** ~{location.estimated_tokens}

Read this section using:
```python
Read("{location.file}", offset={location.start_line}, limit={location.length})
```

Extract the required fields for `{section_name}` and validate immediately.
If validation fails due to missing fields, expand the read range by 50 lines and retry (progressive enhancement).
""")

    sections_text = "\n".join(section_instructions)

    prompt = f"""# Phase 2 Enhanced: Extract Financial Data for {issuer_name}

## TARGETED SECTION EXTRACTION

This extraction uses **indexed sections** instead of reading entire files.

**Token Efficiency:**
- Full files: ~140,000 tokens
- Indexed sections: ~{indexer.get_total_estimated_tokens():,} tokens
- **Savings: {(1 - indexer.get_total_estimated_tokens()/140000)*100:.1f}%**

**Sections Found:** {len(indexer.index)}/{len(indexer.SECTION_MARKERS)}

---

## EXTRACTION WORKFLOW

{sections_text}

---

## VALIDATION REQUIREMENTS

After extracting each section, validate:

**Balance Sheet:**
- Required: total_assets, mortgages_noncurrent, mortgages_current, credit_facilities, cash
- Check: Assets ‚âà Liabilities + Equity (within 1%)

**Income Statement:**
- Required: noi, interest_expense, revenue
- Check: NOI < Revenue, NOI margin 40-70%

**FFO/AFFO:**
- Required: ffo, affo, ffo_per_unit, affo_per_unit, distributions_per_unit
- Check: AFFO < FFO, payout ratios reasonable

**Portfolio:**
- Required: total_properties, total_gla_sf, occupancy_rate
- Check: Occupancy between 0.0-1.0 (decimal)

If validation fails, use **progressive enhancement**: expand read range by 50 lines and retry (max 3 attempts).

---

## CHECKPOINTING

Save each validated section to:
`{output_path.parent}/checkpoints/{{section_name}}.json`

This enables resuming failed extractions without re-reading successful sections.

---

## FINAL ASSEMBLY

Once all sections are extracted and validated, assemble into final JSON:

```json
{{
  "issuer_name": "{issuer_name}",
  "reporting_date": "YYYY-MM-DD",
  "currency": "CAD",
  "balance_sheet": {{ ... }},
  "income_statement": {{ ... }},
  "ffo_affo": {{ ... }},
  "portfolio": {{ ... }},
  "cash_flow_investing": {{ ... }},
  "cash_flow_financing": {{ ... }},
  "liquidity": {{ ... }}
}}
```

**Save to:** `{output_path}`

---

## SUCCESS CRITERIA

- ‚úÖ All required sections extracted
- ‚úÖ All validations passed
- ‚úÖ Final JSON saved
- ‚úÖ Total tokens < 20,000
"""

    return prompt


def create_extraction_roadmap(
    markdown_files: List[Path],
    issuer_name: str,
    output_dir: Path
) -> tuple[ExtractionIndexer, str]:
    """
    Create extraction roadmap using indexer

    Phase 2a: Index sections (0 LLM tokens)
    Phase 2b: Generate targeted extraction prompt

    Args:
        markdown_files: List of markdown files to index
        issuer_name: Name of issuer
        output_dir: Output directory

    Returns:
        (ExtractionIndexer, prompt_text)
    """
    print("\n" + "=" * 70)
    print("PHASE 2 ENHANCED: TARGETED SECTION EXTRACTION")
    print("=" * 70)

    # Phase 2a: Create index (0 tokens)
    print("\nüìç Phase 2a: Creating section index (grep-based, 0 tokens)...")
    indexer = ExtractionIndexer(markdown_files)

    # Check for cached index
    index_cache = output_dir / "section_index.json"
    if index_cache.exists():
        print(f"   üíæ Found cached index: {index_cache}")
        indexer = ExtractionIndexer.load_index(index_cache, markdown_files)
    else:
        indexer.create_index(save_to=index_cache)

    # Show token savings
    total_tokens = indexer.get_total_estimated_tokens()
    savings = (1 - total_tokens/140000) * 100
    print(f"\nüí∞ Token Efficiency:")
    print(f"   Full files: ~140,000 tokens")
    print(f"   Indexed sections: ~{total_tokens:,} tokens")
    print(f"   Savings: {savings:.1f}%\n")

    # Phase 2b: Generate prompt
    print("üìù Phase 2b: Generating targeted extraction prompt...")
    output_path = output_dir / "phase2_extracted_data.json"
    prompt = create_enhanced_extraction_prompt(indexer, issuer_name, output_path)

    return indexer, prompt


def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(
        description="Phase 2 Enhanced: Targeted section extraction"
    )
    parser.add_argument(
        'markdown_files',
        nargs='+',
        type=Path,
        help="Markdown files to extract from"
    )
    parser.add_argument(
        '--issuer-name',
        required=True,
        help="Name of issuer"
    )
    parser.add_argument(
        '--output-dir',
        type=Path,
        default=None,
        help="Output directory (default: same as first markdown file)"
    )

    args = parser.parse_args()

    # Determine output directory
    if args.output_dir:
        output_dir = args.output_dir
    else:
        # Use parent directory of first markdown file
        output_dir = args.markdown_files[0].parent.parent

    output_dir.mkdir(parents=True, exist_ok=True)

    # Create extraction roadmap
    indexer, prompt = create_extraction_roadmap(
        args.markdown_files,
        args.issuer_name,
        output_dir
    )

    # Save prompt for Claude Code
    prompt_path = output_dir / "phase2_extraction_prompt_v2.txt"
    with open(prompt_path, 'w') as f:
        f.write(prompt)

    print(f"\n‚úÖ Extraction prompt saved: {prompt_path}")
    print(f"   Prompt size: {len(prompt):,} characters (~{len(prompt)//4:,} tokens)")
    print(f"   üéØ Total extraction cost: ~{indexer.get_total_estimated_tokens():,} tokens")
    print("\n" + "=" * 70)
    print("NEXT STEPS")
    print("=" * 70)
    print("\nüìã Claude Code will now:")
    print("   1. Read the extraction prompt")
    print("   2. For each section, read ONLY the indexed lines")
    print("   3. Extract and validate each section")
    print("   4. Use progressive enhancement if needed")
    print("   5. Save checkpoints after each section")
    print("   6. Assemble final JSON")
    print("\n‚è≥ Ready for Claude Code extraction...")
    print("=" * 70 + "\n")


if __name__ == '__main__':
    main()
